# Edit this file to introduce tasks to be run by cron.
# 
# Each task to run has to be defined through a single line
# indicating with different fields when the task will be run
# and what command to run for the task
# 
# To define the time you can provide concrete values for
# minute (m), hour (h), day of month (dom), month (mon),
# and day of week (dow) or use '*' in these fields (for 'any').
# 
# Notice that tasks will be started based on the cron's system
# daemon's notion of time and timezones.
# 
# Output of the crontab jobs (including errors) is sent through
# email to the user the crontab file belongs to (unless redirected).
# 
# For example, you can run a backup of all your user accounts
# at 5 a.m every week with:
# 0 5 * * 1 tar -zcf /var/backups/home.tgz /home/
# 
# For more information see the manual pages of crontab(5) and cron(8)
# 
# m h  dom mon dow   command
0 1 * * * /usr/bin/docker run --network=aps_default --env TZ=Asia/Tokyo -d aps-crawler python main.py netsea -i discount
0 2 * * * /usr/bin/docker run --network=aps_default --env TZ=Asia/Tokyo -d aps-crawler python main.py super -i discount
0 4 * * * /usr/bin/docker run --network=aps_default --env TZ=Asia/Tokyo -d aps-crawler python main.py super -i bookmark
0 5 * * * /usr/bin/docker run --network=aps_default --env TZ=Asia/Tokyo -d aps-crawler python main.py super -i new
0 8 * * * /usr/bin/docker run --network=aps_default --env TZ=Asia/Tokyo -d aps-crawler python main.py netsea -i bookmark
0 9 * * * /usr/bin/docker run --network=aps_default --env TZ=Asia/Tokyo -d aps-crawler python main.py netsea -i new
0 16 * * * /usr/bin/docker run --network=aps_default --env TZ=Asia/Tokyo -d aps-crawler python main.py pcones
0 18 * * * /usr/bin/docker run --network=aps_default --env TZ=Asia/Tokyo -d aps-crawler python main.py spread_sheet
#
0 1 * * * /usr/bin/docker compose -f /home/jun/aps/compose.yaml run -d go-crawler /app/go-crawler -s ikebe -u "https://www.ikebe-gakki.com/Form/Product/ProductList.aspx?shop=0&cat=&bid=ec&dpcnt=40&img=1&sort=07&udns=1&fpfl=0&sfl=0&pno=1"
0 2 * * 0 /usr/bin/docker compose -f /home/jun/aps/compose.yaml run -d go-crawler /app/go-crawler -s kaago -u "https://kaago.com/ajax/catalog/list/init"
0 4 * * 6 /usr/bin/docker compose -f /home/jun/aps/compose.yaml run -d go-crawler /app/go-crawler -s rakuten -i all
0 4 * * 0-5 /usr/bin/docker compose -f /home/jun/aps/compose.yaml run -d go-crawler /app/go-crawler -s rakuten -i daily
0 15 * * * /usr/bin/docker compose -f /home/jun/aps/compose.yaml run -d go-crawler /app/go-crawler -s hikaritv -u "https://shop.hikaritv.net/shopping/app/catalog/list/init?searchCategoryCode=0&searchWord=&searchCommodityCode=&searchMethod=0&searchType=0&squeezeSerch=0&hideKeyWord=&hidePriceMin=&hidePriceMax=50000&keywordToggle=&alignmentSequence=3&pageSize=50&mode=image&pageLayout=window&searchMakerName=&pointFacet=&discountRateFacet=&searchPriceStart=&searchPriceEnd=&searchTagCode=&searchCouponCode=&fqGetPoint=&fqStartDateMin=&fqStartDateMax=&fqStartDateName=&fqAverageRating=&banner=&notDisplayFacet=&currentPage=1&fqStockStatus=1"
0 17 * * * /usr/bin/docker compose -f /home/jun/aps/compose.yaml run -d go-crawler /app/go-crawler -s pc4u -u "https://www.pc4u.co.jp/view/category/outlet"
0 17 * * * /usr/bin/docker compose -f /home/jun/aps/compose.yaml run -d go-crawler /app/go-crawler -s nojima -u "https://online.nojima.co.jp/app/catalog/list/init?searchCategoryCode=0&mode=image&pageSize=60&currentPage=1&alignmentSequence=9&searchDispFlg=true&immediateDeliveryDispFlg=1&searchWord=+"
30 17 * * * /usr/bin/docker compose -f /home/jun/aps/compose.yaml run -d go-crawler /app/go-crawler -s pc4u -u "https://www.pc4u.co.jp/view/search"
0 18 * * * /usr/bin/docker compose -f /home/jun/aps/compose.yaml run -d go-crawler /app/go-crawler -s ark -u "https://www.ark-pc.co.jp/search/?limit=50&nouki=1"
#
*/10 * * * * /usr/bin/docker compose -f /home/jun/aps/compose.yaml run --rm api-server /app/delete_row
0 * * * * /usr/bin/docker container ls -a | grep -i 'exited (0)' | awk '{print $1}' | xargs /usr/bin/docker rm
0 * * * * curl -X POST 'http://localhost/api/inventory/refresh'
0 3 * * * /usr/bin/docker run --network=aps_default --env TZ=Asia/Tokyo -d aps-crawler python main.py spapi -i register
0 4 * * * /usr/bin/docker run --network=aps_default --env TZ=Asia/Tokyo -d aps-crawler python main.py spapi -i check
5 0 * * * path=/home/jun/backup/`date +\%Y\%m\%d` && pg_basebackup -D $path -h localhost -U postgres -c fast -P && tar c -C /home/jun/backup `date +\%Y\%m\%d` | pzstd -p 12 > "$path".tar.zst && rm -r $path
2 0 * * * find /home/jun/backup -type f -mtime +14 | sort | xargs rm
